#Enter your whisper model, see VRAM requirement for further details at whisper Github | tiny, base, small, tiny.en, base.en
WHISPER_MODEL = base

#Use the (roughly 4x faster) whisper-turbo library. Requires CUDA Toolkit and cuDNN downloaded to work with GPU.
FASTER_WHISPER = OFF
FASTER_WHISPER_CPU_TRANSCRIPTION = OFF

#Make the speech to text split into chunks and calculate as you speak? (Lower = less percise, but calculates more often. 300 - 1200 is resonable)
WHISPER_CHUNKY = ON
WHISPER_CHUNKY_RATE = 710
WHISPER_CHUNKS_MAX = 14

#Name that you want your bot/waifu to have (used in like 2 places, unimportant)
CHAR_NAME = Waifu

#Use the name of the Ooobabooga character card you want.
CHARACTER_CARD = Waifu

#Enter your name that you entered while creating Character card. Typically User, You or if have entered your name.
YOUR_NAME = You

#Decide if hotkeys should be on or off when the program/waifu first boots. Valid values are "ON" and "OFF"
HOTKEYS_BOOT = OFF

#Decide if "Newline Cut" and "RP Supression" should start "ON" or "OFF".
NEWLINE_CUT_BOOT = OFF
RP_SUP_BOOT = ON

#API type (either Oobabooga or Ollama. For other Openai-styled endpoints, do Oobabooga)
API_TYPE = Oobabooga
API_TYPE_VISUAL = Ollama

#Ollama Options. A visual model only runs when using multimodal / when sending images. If pointed at the same model, it can be used for both (saves VRAM).
#Visual model only loaded if MODULE_VISUAL is "ON".
ZW_OLLAMA_MODEL = "nsheth/llama-3-lumimaid-8b-v0.1-iq-imatrix"
ZW_OLLAMA_MODEL_VISUAL = "nsheth/llama-3-lumimaid-8b-v0.1-iq-imatrix"

#Use the streaming API for Oobabooga/Ollama? Disable this if you are having issues getting responses, or the voice is glitchy while generating.
API_STREAM_CHATS = ON

# Enable / Disable using filler words while the responses are first booting up (API_STREAM_CHATS must be on)
USE_CHATPOPS = ON

#Maximum context length to tell the model to look at. More takes more VRAM and longer to generate, generally.
#TOKEN_LIMIT is for the actual ML portion and the actual cutoff, MESSAGE_PAIR_LIMIT just decides how much to send to it.
TOKEN_LIMIT = 4096
MESSAGE_PAIR_LIMIT = 30

#Share the current time with the bot?
TIME_IN_ENCODING = ON

#Info for the model link
#Works with any OpenAI-compatible LLM server, such as oobabooga/text-generation-webui, ollama in server mode, or other OpenAI endpoints.
HOST_PORT = 127.0.0.1:5000

# Visual model info
IMG_PORT = 127.0.0.1:5007
OOBA_VISUAL_CHARACTER_NAME = Z-WAIF-VisualAssist
OOBA_VISUAL_PRESET_NAME = Z-WAIF-VisualPreset
OLLAMA_VISUAL_ENCODE_GUIDANCE = ON
# Valid values for the character card are: "VISUAL", "BASE", "OFF"
OLLAMA_VISUAL_CARD = VISUAL

#How should we scale the image? 1.0 = Small (for Oobabooga), 1.6 = Big (for Ollama). Sane values are 0.4 - 4.0.
IMG_SCALE = 1.0

#What you want your VTuber's eyes to move to. Default gives control to the model. Values can be: "Faces", "Random", "None". Requires setting up in the model.
EYES_FOLLOW = "None"
EYES_START_ID = 14

#Mininum recording time for an autochat to actually register
AUTOCHAT_MIN_LENGTH = 157

#Should we use Silero VAD for our Autochat, or just the normal volume based one? Options = "ON", "OFF"
SILERO_VAD = ON

#Decides of each of these modules should be running. Recommended to turn on RAG after a few hours of use, for better memory. Valid "ON" or "OFF".
MODULE_MINECRAFT = OFF
MODULE_GAMING = OFF
MODULE_ALARM = OFF
MODULE_VTUBE = OFF
MODULE_DISCORD = OFF
MODULE_RAG = OFF
MODULE_VISUAL = OFF
